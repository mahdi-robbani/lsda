{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"lsda","language":"python","name":"lsda"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Assignment3-Ex3.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"I8iyBhToGGuk","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# generate data\n","signal_length = 15\n","signal_repeats = 3\n","predict_ahead = 1\n","noise_strength = 0.00\n","total_series_length = signal_length * signal_repeats\n","\n","dims = 1\n","def generateData(signal_length, predict_ahead, signal_repeats, batch_size, noise_strength):\n","    total_series_length = signal_length*signal_repeats\n","    x = np.linspace(0, np.pi*2*signal_repeats, total_series_length + predict_ahead, dtype=np.float32)\n","    x = x.reshape((1, -1, 1))  \n","    \n","    # include shift for batches\n","    x = np.repeat(x, batch_size, 0)\n","    x += np.random.random(batch_size)[:, None, None] * 10\n","    y = np.sin(x)\n","    input_ = y[:, :total_series_length].copy()\n","    if noise_strength > 0:\n","        input_ += np.random.normal(size=(input_.shape)) * noise_strength # add some noise to the input\n","    target = y[:, predict_ahead:]\n","    return input_ , target \n","x, y = generateData(signal_length, predict_ahead, signal_repeats, 1, noise_strength)\n","# x_i, y_i = x[0, :, 0], y[0, :, 0]\n","# f, ax = plt.subplots(1)\n","# ax.plot(x_i, label=\"input\")\n","# ax.plot(y_i, label=\"target\")\n","# ax.set_xlabel(\"time $t$\")\n","# ax.set_ylabel(\"signal magnitude\")\n","# ax.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tw0mHPaHGGut","colab_type":"code","colab":{}},"source":["#plotting helpter function\n","# just a helper function to see the progress\n","def replot(input_series, predictions_series, batchY, f, ax,  input_line, y_line, pred_line, text, iteration, loss):\n","    y_line.set_xdata(range(total_series_length))\n","    y_line.set_ydata(batchY)\n","    pred_line.set_xdata(range(total_series_length))\n","    pred_line.set_ydata(predictions_series)\n","    input_line.set_xdata(range(total_series_length))\n","    input_line.set_ydata(input_series)\n","    text.set_text(f\"Iteration: {iteration} Loss: {loss}\")\n","#     plt.pause(0.25)\n","    \n","    # Need both of these in order to rescale\n","    ax.relim()\n","    ax.autoscale_view()\n","    # We need to draw and flush\n","    f.canvas.draw()\n","    f.canvas.flush_events()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZD0OGGPGGuy","colab_type":"text"},"source":["# 3 Recurrent Neural Networks (30 pts.)\n","Replace the simple recurrent layer in the `LSDA2020 RNN1.ipynb` notebook with a LSTM\n","layer. Follow the equations from `LSDA2020 RNN2.ipynb` to implement the LSTM. You will\n","need to:\n","1. (10 pts.) add all variables (`tf.Variable`) from the LSTM definition\n","2. (10 pts.) extend the step and `iterate_series` function\n","3. (10 pts.) pass on the hidden state $h_t$ and the cell state ct in the training loop andthe `iterate_series` function\n","Report the new code in the report (insert all changed code cells from the notebook into\n","the report). Train the new model, **report the training loss and also attach the figure of\n","the learned prediction.**\n","Note: You can initialize the cell state c0 to zero (same as for h0) and do not need to\n","learn it."]},{"cell_type":"code","metadata":{"id":"eHc1I3C3GGuz","colab_type":"code","colab":{}},"source":["#3.1\n","#Learnable weights and biases\n","rnn_mode = True\n","learn_h0 = True\n","n_iterations = 250\n","truncated_backprop_length = 45\n","num_neurons = 8\n","batch_size = 5\n","lr = 0.01"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZulfhGXTGGu3","colab_type":"code","colab":{}},"source":["W_f = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n","b_f = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n","U_f = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n","W_i = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n","b_i = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n","U_i = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n","W_j = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n","b_j = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n","U_j = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n","W_o = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n","b_o = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n","U_o = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n","# if rnn_mode:\n","#     U_h = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n","# else:\n","#     U_h = tf.Variable(np.zeros((num_neurons, num_neurons)), dtype=tf.float32)\n","\n","# initial hidden state\n","h_0 = tf.Variable(np.zeros((1, num_neurons), dtype=np.float32))\n","c_0 = tf.Variable(np.zeros((1, num_neurons), dtype=np.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMcb77MSGGu6","colab_type":"code","colab":{}},"source":["# fully connected layer\n","W_y = tf.Variable(np.random.rand(num_neurons, dims), dtype=tf.float32)\n","b_y = tf.Variable(np.zeros((1, dims)), dtype=tf.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ02d2v0GGu9","colab_type":"text"},"source":["RNN:\n","$ \\mathbf{h}_t \\leftarrow \\sigma_h \\bigl( W_h \\cdot \\mathbf{x}_t + U_h \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_h \\bigr) $\n","\n","$ \\hat{\\mathbf{y}}_t \\leftarrow \\sigma_y \\bigl( W_y \\cdot \\mathbf{h}_t + \\mathbf{b}_y \\bigr) $\n","LSTM:\n","## **Definition**\n","\n","|||\n","|-------------|---------------------------------------------------------------------|\n","|hidden state | $\\mathbf{h}_t \\leftarrow \\mathbf{o}_t \\circ \\sigma_h(\\mathbf{c}_t)$ |\n","|cell state   | $\\mathbf{c}_t \\leftarrow \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{j}_t $ |\n","|output gate  | $\\mathbf{o}_t \\leftarrow \\sigma_o(W_o \\cdot \\mathbf{x}_t + U_o \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_o)$ |\n","|forget gate  | $\\mathbf{f}_t \\leftarrow \\sigma_f(W_f \\cdot \\mathbf{x}_t + U_f \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_f)$ |\n","|input gate   | $\\mathbf{i}_t \\leftarrow \\sigma_i(W_i \\cdot \\mathbf{x}_t + U_i \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_i)$|\n","|modulation gate| $\\mathbf{j}_t \\leftarrow \\sigma_j(W_j \\cdot \\mathbf{x}_t + U_j \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_j)$ |\n","\n","- $W \\in \\mathbb{R}^{m \\times d}$ weight matrices for input vector $\\mathbf{x}_t$\n","- $U \\in \\mathbb{R}^{m \\times m}$ weight matrices for hidden state vector $\\mathbf{h}_{t-1}$\n","- activation functions $\\sigma_h$ and $\\sigma_j$ are usually $tanh$\n","- activation functions $\\sigma_f$, $\\sigma_i$, and $\\sigma_o$ are usually $sigmoid$\n","- element-wise multiplication is $\\circ$\n","    * important for gating (all values between usually between -1 and 1 (tanh) or 0 and 1 (sigmoid)\n","- cell state $\\mathbf{c}$ : stores contextual and longer term information\n","- hidden state $\\mathbf{h}$ : stores immediately necessary information and **is given to next layer**"]},{"cell_type":"code","metadata":{"id":"40Ie1mSvGGu-","colab_type":"code","colab":{}},"source":["@tf.function\n","def step(x_t, h, c):\n","    # forget layer\n","    f = tf.nn.sigmoid(\n","        tf.matmul(x_t, W_f) + tf.matmul(h, U_f) + b_f\n","    )\n","    # input layer:\n","    i = tf.nn.sigmoid(\n","        tf.matmul(x_t, W_i) + tf.matmul(h, U_i) + b_i\n","    )\n","    # modulation layer:\n","    j = tf.nn.tanh(\n","        tf.matmul(x_t, W_j) + tf.matmul(h, U_j) + b_j\n","    )\n","    # output layer:\n","    o = tf.nn.sigmoid(\n","        tf.matmul(x_t, W_o) + tf.matmul(h, U_o) + b_o\n","    )\n","    # memory (cell) layer:\n","    c = f*c + i*j\n","    #c = tf.matmul(c, f) + tf.matmul(j, i)\n","    # hidden state \n","    h = o * tf.nn.tanh(c)\n","    #h = tf.matmul(o, tf.nn.tanh(c))\n","    y_hat = h\n","    return y_hat, h, c\n","\n","@tf.function\n","def iterate_series(x, h, c):\n","    y_hat = []\n","    # iterate over time axis (1)\n","    for j in range(x.shape[1]):\n","        # give previous hidden state and input from the current time step\n","        y_hat_t, h, c = step(x[:, j], h, c)\n","        y_hat.append(y_hat_t)\n","    y_hat = tf.stack(y_hat, 1)\n","    return y_hat, h, c\n","\n","# @tf.function\n","# def step(x_t, h):\n","#     # rnn layer\n","#     h = tf.nn.tanh(\n","#         tf.matmul(x_t, W_h) + tf.matmul(h, U_h) + b_h\n","#     )\n","#     # fully connected\n","#     y_hat = tf.matmul(h, W_y) + b_y\n","#     return y_hat, h #returning prediction and hidden state\n","\n","# @tf.function\n","# def iterate_series(x, h):\n","#     y_hat = []\n","#     # iterate over time axis (1)\n","#     for j in range(x.shape[1]):\n","#         # give previous hidden state and input from the current time step\n","#         y_hat_t, h = step(x[:, j], h)\n","#         y_hat.append(y_hat_t)\n","#     y_hat = tf.stack(y_hat, 1)\n","#     return y_hat, h"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DvOcaeuGGvC","colab_type":"code","colab":{}},"source":["# backpropagation\n","trainable_vars = [W_f, U_f, b_f, W_i, U_i, b_i, W_j, U_j, b_j, W_o, U_o, b_o]\n","#trainable_vars.append(h_0, c_0)\n","\n","#trainable_vars = [W_h, U_h, b_h, W_y, b_y] #*for rnn\n","# if rnn_mode:\n","#     trainable_vars.append(U_h)\n","# if learn_h0:\n","#     trainable_vars.append(h_0)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyZiE_KfGGvG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"5ffb69b9-47c4-4362-95e1-ee74e563f3a8","executionInfo":{"status":"ok","timestamp":1590347601475,"user_tz":-120,"elapsed":9875,"user":{"displayName":"Mahdi Robbani","photoUrl":"https://lh6.googleusercontent.com/-7lZA3if99HM/AAAAAAAAAAI/AAAAAAAAkec/9eSirTq-bDU/s64/photo.jpg","userId":"13401353975772030318"}}},"source":["# training\n","plt.ion()\n","f, ax = plt.subplots(1)\n","y_line, = ax.plot([],[], color=\"blue\", label=\"target $y$\")\n","pred_line, = ax.plot([],[], color=\"red\", label=\"prediction $\\haty$\")\n","input_line, = ax.plot([],[], color=\"green\", label=\"input $x$\")\n","text = ax.text(0, 1, \"\", )\n","ax.legend(loc='upper right')\n","\n","# train for a set number of iterations\n","for iteration in range(n_iterations):\n","    # generates a long time series / normally loaded from dataset (e.g. stocks, weather)\n","    x, y = generateData(signal_length, predict_ahead, signal_repeats, batch_size, noise_strength)\n","    h = None\n","    predictions = None\n","    loss_list = []\n","    grads = None\n","    \n","    # do not feed complete series, but chunks of it (truncated_backprop_length)\n","    for i in range(0, total_series_length, truncated_backprop_length):\n","        \n","        with tf.GradientTape() as tape:\n","            if h is None:\n","                # initialize hidden state (h_0) -> new shape (batch_size, num_neurons)\n","                h = tf.repeat(h_0, batch_size, 0)\n","                c = tf.repeat(c_0, batch_size, 0) #added\n","            x_part = x[:, i: i + truncated_backprop_length]\n","            y_part = y[:, i: i + truncated_backprop_length]\n","            \n","            # get predictions for this part (forward pass)\n","            #y_hat, h = iterate_series(x_part, h)\n","            y_hat, h, c = iterate_series(x_part, h, c)\n","        \n","            # calculate mean squared error\n","            loss = tf.reduce_mean((y_hat - y_part)**2)        \n","        # backprop\n","        if grads is None:\n","            grads = tape.gradient(loss, trainable_vars)\n","        else:\n","            grads = grads + tape.gradient(loss, trainable_vars)\n","                           \n","        loss_list.append(loss)\n","        # combine with previous predictions\n","        predictions = tf.concat([predictions, y_hat], 1) if predictions is not None else y_hat\n","    \n","    # finally we are adapting the weights\n","    optimizer.apply_gradients(zip(grads, trainable_vars))\n","        \n","#     print(\"Iteration\", iteration, \"Loss\", np.mean(loss_list))  \n","    #replot(x[0], predictions[0], y[0], f, ax, input_line, y_line, pred_line, text, iteration, np.mean(loss_list))"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"w6e_57l9GGvJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2og_RuN3GGvL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hQ-0FOnGGvN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1KubG9SGGvP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0ViETuRGGvR","colab_type":"text"},"source":["# Recurrent Neural Networks in Keras (30 pts.)\n","Add different components to the RNN from LSDA2020 RNN2.ipynb and test the new mod-\n","els (each bullet point is a new separate model). The number of neuron can stay the same\n","for all layers (default was 64). Report the mean validation error and the changed parts\n","with a bit of context in the code (e.g. complete model or optimizer definition). Also,\n","provide a model overview with model.summary() for each new model (not necessary for\n","gradient clipping).\n","1. (7 pts.) Replace the LSTM layer with a Bidirectional-LSTM. Bidirectional sequence processing is possible with tf.keras.layers.Bidirectional.\n","2. (4 pts.) Stack 2 LSTM layers (you may need to use the return sequences parameter).\n"," * (4 pts.) Detail the difference to bidirectional processing (max. 2-3 sentences).\n","3. (7 pts.) Add a 1-d convolution layer (tf.keras.layers.Conv1D) with a kernel size of 3 and stride of 1 before the recurrent part. You will need to reshape the data with tf.keras.layers.Reshape.\n"," * (5 pts.) Explain the difference between how a convolutional layer process time series and how a recurrent model does it (max 4-5 sentences).\n","4. (3 pts.) Gradient clipping can be a helpful to train recurrent networks. Keras offers to clip gradients directly through the optimizer. Change the optimizer to clip the gradients to 1.\n","It is more important here to show that you correctly changed the model (by showing the\n","corresponding code in the report) or optimizer than improving the results."]},{"cell_type":"code","metadata":{"id":"yRT-LRwAGGvS","colab_type":"code","colab":{}},"source":["# import some layer\n","from tensorflow.keras.layers import SimpleRNN, Input, Dense, LSTM, GRU, AveragePooling1D, TimeDistributed, Bidirectional, Conv1D, Flatten, Layer, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import activations, initializers\n","INPUT_SHAPE = (100, 22)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"orE5BwwfGGvV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":259},"outputId":"89d250bf-d8ae-4aa8-a270-298d6f694e15","executionInfo":{"status":"ok","timestamp":1590347602021,"user_tz":-120,"elapsed":10374,"user":{"displayName":"Mahdi Robbani","photoUrl":"https://lh6.googleusercontent.com/-7lZA3if99HM/AAAAAAAAAAI/AAAAAAAAkec/9eSirTq-bDU/s64/photo.jpg","userId":"13401353975772030318"}}},"source":["# Exercise 4.1\n","def get_model_bidirectional(shape= INPUT_SHAPE):\n","    inp = Input(shape)\n","    x = Bidirectional(LSTM(64))(inp)\n","    x = Dense(1)(x)\n","    model = Model(inp, x)\n","    return model\n","\n","get_model_bidirectional().summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 100, 22)]         0         \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 128)               44544     \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 129       \n","=================================================================\n","Total params: 44,673\n","Trainable params: 44,673\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DSRBJ_wgGGvY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":294},"outputId":"8be85e39-ee6f-4fb7-8606-ba3c32a3af7a","executionInfo":{"status":"ok","timestamp":1590347602402,"user_tz":-120,"elapsed":10750,"user":{"displayName":"Mahdi Robbani","photoUrl":"https://lh6.googleusercontent.com/-7lZA3if99HM/AAAAAAAAAAI/AAAAAAAAkec/9eSirTq-bDU/s64/photo.jpg","userId":"13401353975772030318"}}},"source":["# Exercise 4.2\n","def get_stacked_LSTM(shape= INPUT_SHAPE):\n","    inp = Input(shape)\n","    x = LSTM(64, return_sequences=True)(inp)\n","    x = LSTM(64, go_backwards=True)(x)\n","    x = Dense(1)(x)\n","    model = Model(inp, x)\n","    return model\n","\n","get_stacked_LSTM().summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 100, 22)]         0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100, 64)           22272     \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 64)                33024     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 55,361\n","Trainable params: 55,361\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9_PFBAmyGGva","colab_type":"text"},"source":["##  Detail the difference to bidirectional processing (max. 2-3 sentences\n","The bidirectional layers outputs two concatenated LSTM layers, while the stacked layers output one single LSTM layer. In the stacked layer, the input of the backward layer is affected by the forward layer since it passes through the forward lyer, but the two layers are indepdedant in the bidirectional layer."]},{"cell_type":"code","metadata":{"id":"Mw7SihqKGGva","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":294},"outputId":"eec2a49a-c4a4-4fb2-e58b-450deb578f6b","executionInfo":{"status":"ok","timestamp":1590347602693,"user_tz":-120,"elapsed":11037,"user":{"displayName":"Mahdi Robbani","photoUrl":"https://lh6.googleusercontent.com/-7lZA3if99HM/AAAAAAAAAAI/AAAAAAAAkec/9eSirTq-bDU/s64/photo.jpg","userId":"13401353975772030318"}}},"source":["#Add a 1-d convolution layer (tf.keras.layers.Conv1D) with a kernel size of 3 and stride of 1 before the recurrent part. \n","#You will need to reshape the data with tf.keras.layers.Reshape. ??????????\n","# Exercise 4.3\n","def get_model_convolutional(shape = INPUT_SHAPE):\n","    inp = Input(shape)\n","    x = Conv1D(filters=32, kernel_size=3, strides=1)(inp)\n","    x = LSTM(64, return_sequences=False)(x)\n","    x = Dense(1)(x)\n","    model = Model(inp, x)\n","    return model\n","get_model_convolutional().summary()\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 100, 22)]         0         \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 98, 32)            2144      \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 64)                24832     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 27,041\n","Trainable params: 27,041\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yRenbmebGGvc","colab_type":"text"},"source":["A recurrent nueral network uses every piece of the input data to make a predictiion. A convolutional neural network extracts features from the input data a subset of the input data and uses the features to make a prediction. Not all input data may be necessary to makea good prediction therefore CNNs can be less comoutationally exensive."]},{"cell_type":"code","metadata":{"id":"F2nzJ0t_GGvd","colab_type":"code","colab":{}},"source":["# Exercise 4.4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr, clipvalue=1)\n","get_model_convolutional().compile(optimizer, loss='mae')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"arT-loHiGGvf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Q8Q33RfGGvh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RqTW5m8GGvi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlDhw3jpGGvk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":259},"outputId":"76429264-8741-4910-a210-c7fff4cabe5e","executionInfo":{"status":"ok","timestamp":1590347603344,"user_tz":-120,"elapsed":11666,"user":{"displayName":"Mahdi Robbani","photoUrl":"https://lh6.googleusercontent.com/-7lZA3if99HM/AAAAAAAAAAI/AAAAAAAAkec/9eSirTq-bDU/s64/photo.jpg","userId":"13401353975772030318"}}},"source":["def get_model_rnn(shape = INPUT_SHAPE):\n","    inp = Input(shape)\n","    x = LSTM(64, return_sequences=False)(inp)\n","    x = Dense(1)(x)\n","    model = Model(inp, x)\n","    return model\n","get_model_rnn().summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         [(None, 100, 22)]         0         \n","_________________________________________________________________\n","lstm_5 (LSTM)                (None, 64)                22272     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 22,337\n","Trainable params: 22,337\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CpgPc0P-GGvm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}