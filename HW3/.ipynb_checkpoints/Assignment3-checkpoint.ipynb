{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate data\n",
    "signal_length = 15\n",
    "signal_repeats = 3\n",
    "predict_ahead = 1\n",
    "noise_strength = 0.00\n",
    "total_series_length = signal_length * signal_repeats\n",
    "\n",
    "dims = 1\n",
    "def generateData(signal_length, predict_ahead, signal_repeats, batch_size, noise_strength):\n",
    "    total_series_length = signal_length*signal_repeats\n",
    "    x = np.linspace(0, np.pi*2*signal_repeats, total_series_length + predict_ahead, dtype=np.float32)\n",
    "    x = x.reshape((1, -1, 1))  \n",
    "    \n",
    "    # include shift for batches\n",
    "    x = np.repeat(x, batch_size, 0)\n",
    "    x += np.random.random(batch_size)[:, None, None] * 10\n",
    "    y = np.sin(x)\n",
    "    input_ = y[:, :total_series_length].copy()\n",
    "    if noise_strength > 0:\n",
    "        input_ += np.random.normal(size=(input_.shape)) * noise_strength # add some noise to the input\n",
    "    target = y[:, predict_ahead:]\n",
    "    return input_ , target \n",
    "x, y = generateData(signal_length, predict_ahead, signal_repeats, 1, noise_strength)\n",
    "# x_i, y_i = x[0, :, 0], y[0, :, 0]\n",
    "# f, ax = plt.subplots(1)\n",
    "# ax.plot(x_i, label=\"input\")\n",
    "# ax.plot(y_i, label=\"target\")\n",
    "# ax.set_xlabel(\"time $t$\")\n",
    "# ax.set_ylabel(\"signal magnitude\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting helpter function\n",
    "# just a helper function to see the progress\n",
    "def replot(input_series, predictions_series, batchY, f, ax,  input_line, y_line, pred_line, text, iteration, loss):\n",
    "    y_line.set_xdata(range(total_series_length))\n",
    "    y_line.set_ydata(batchY)\n",
    "    pred_line.set_xdata(range(total_series_length))\n",
    "    pred_line.set_ydata(predictions_series)\n",
    "    input_line.set_xdata(range(total_series_length))\n",
    "    input_line.set_ydata(input_series)\n",
    "    text.set_text(f\"Iteration: {iteration} Loss: {loss}\")\n",
    "#     plt.pause(0.25)\n",
    "    \n",
    "    # Need both of these in order to rescale\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    # We need to draw and flush\n",
    "    f.canvas.draw()\n",
    "    f.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Recurrent Neural Networks (30 pts.)\n",
    "Replace the simple recurrent layer in the `LSDA2020 RNN1.ipynb` notebook with a LSTM\n",
    "layer. Follow the equations from `LSDA2020 RNN2.ipynb` to implement the LSTM. You will\n",
    "need to:\n",
    "1. (10 pts.) add all variables (`tf.Variable`) from the LSTM definition\n",
    "2. (10 pts.) extend the step and `iterate_series` function\n",
    "3. (10 pts.) pass on the hidden state $h_t$ and the cell state ct in the training loop andthe `iterate_series` function\n",
    "Report the new code in the report (insert all changed code cells from the notebook into\n",
    "the report). Train the new model, **report the training loss and also attach the figure of\n",
    "the learned prediction.**\n",
    "Note: You can initialize the cell state c0 to zero (same as for h0) and do not need to\n",
    "learn it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1\n",
    "#Learnable weights and biases\n",
    "rnn_mode = True\n",
    "learn_h0 = True\n",
    "n_iterations = 250\n",
    "truncated_backprop_length = 45\n",
    "num_neurons = 8\n",
    "batch_size = 5\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_f = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n",
    "b_f = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n",
    "U_f = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n",
    "W_i = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n",
    "b_i = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n",
    "U_i = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n",
    "W_j = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n",
    "b_j = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n",
    "U_j = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n",
    "W_o = tf.Variable(np.random.rand(dims, num_neurons), dtype=tf.float32)\n",
    "b_o = tf.Variable(np.zeros((1, num_neurons)), dtype=tf.float32)\n",
    "U_o = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n",
    "# if rnn_mode:\n",
    "#     U_h = tf.Variable(np.random.rand(num_neurons, num_neurons), dtype=tf.float32)\n",
    "# else:\n",
    "#     U_h = tf.Variable(np.zeros((num_neurons, num_neurons)), dtype=tf.float32)\n",
    "\n",
    "# initial hidden state\n",
    "h_0 = tf.Variable(np.zeros((1, num_neurons), dtype=np.float32))\n",
    "c_0 = tf.Variable(np.zeros((1, num_neurons), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected layer\n",
    "W_y = tf.Variable(np.random.rand(num_neurons, dims), dtype=tf.float32)\n",
    "b_y = tf.Variable(np.zeros((1, dims)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN:\n",
    "$ \\mathbf{h}_t \\leftarrow \\sigma_h \\bigl( W_h \\cdot \\mathbf{x}_t + U_h \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_h \\bigr) $\n",
    "\n",
    "$ \\hat{\\mathbf{y}}_t \\leftarrow \\sigma_y \\bigl( W_y \\cdot \\mathbf{h}_t + \\mathbf{b}_y \\bigr) $\n",
    "LSTM:\n",
    "## **Definition**\n",
    "\n",
    "|||\n",
    "|-------------|---------------------------------------------------------------------|\n",
    "|hidden state | $\\mathbf{h}_t \\leftarrow \\mathbf{o}_t \\circ \\sigma_h(\\mathbf{c}_t)$ |\n",
    "|cell state   | $\\mathbf{c}_t \\leftarrow \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{j}_t $ |\n",
    "|output gate  | $\\mathbf{o}_t \\leftarrow \\sigma_o(W_o \\cdot \\mathbf{x}_t + U_o \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_o)$ |\n",
    "|forget gate  | $\\mathbf{f}_t \\leftarrow \\sigma_f(W_f \\cdot \\mathbf{x}_t + U_f \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_f)$ |\n",
    "|input gate   | $\\mathbf{i}_t \\leftarrow \\sigma_i(W_i \\cdot \\mathbf{x}_t + U_i \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_i)$|\n",
    "|modulation gate| $\\mathbf{j}_t \\leftarrow \\sigma_j(W_j \\cdot \\mathbf{x}_t + U_j \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_j)$ |\n",
    "\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ weight matrices for input vector $\\mathbf{x}_t$\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ weight matrices for hidden state vector $\\mathbf{h}_{t-1}$\n",
    "- activation functions $\\sigma_h$ and $\\sigma_j$ are usually $tanh$\n",
    "- activation functions $\\sigma_f$, $\\sigma_i$, and $\\sigma_o$ are usually $sigmoid$\n",
    "- element-wise multiplication is $\\circ$\n",
    "    * important for gating (all values between usually between -1 and 1 (tanh) or 0 and 1 (sigmoid)\n",
    "- cell state $\\mathbf{c}$ : stores contextual and longer term information\n",
    "- hidden state $\\mathbf{h}$ : stores immediately necessary information and **is given to next layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step(x_t, h, c):\n",
    "    # forget layer\n",
    "    f = tf.nn.sigmoid(\n",
    "        tf.matmul(x_t, W_f) + tf.matmul(h, U_f) + b_f\n",
    "    )\n",
    "    # input layer:\n",
    "    i = tf.nn.sigmoid(\n",
    "        tf.matmul(x_t, W_i) + tf.matmul(h, U_i) + b_i\n",
    "    )\n",
    "    # modulation layer:\n",
    "    j = tf.nn.tanh(\n",
    "        tf.matmul(x_t, W_j) + tf.matmul(h, U_j) + b_j\n",
    "    )\n",
    "    # output layer:\n",
    "    o = tf.nn.sigmoid(\n",
    "        tf.matmul(x_t, W_o) + tf.matmul(h, U_o) + b_o\n",
    "    )\n",
    "    # memory (cell) layer:\n",
    "    #c = f*c + i*j\n",
    "    c = tf.matmul(c, f) + tf.matmul(j, i)\n",
    "    # hidden state \n",
    "    #h = o * tf.nn.tanh(c)\n",
    "    h = tf.matmul(o, tf.nn.tanh(c))\n",
    "    y_hat = h\n",
    "    return y_hat, h, c\n",
    "\n",
    "@tf.function\n",
    "def iterate_series(x, h, c):\n",
    "    y_hat = []\n",
    "    # iterate over time axis (1)\n",
    "    for j in range(x.shape[1]):\n",
    "        # give previous hidden state and input from the current time step\n",
    "        y_hat_t, h, c = step(x[:, j], h, c)\n",
    "        y_hat.append(y_hat_t)\n",
    "    y_hat = tf.stack(y_hat, 1)\n",
    "    return y_hat, h, c\n",
    "\n",
    "# @tf.function\n",
    "# def step(x_t, h):\n",
    "#     # rnn layer\n",
    "#     h = tf.nn.tanh(\n",
    "#         tf.matmul(x_t, W_h) + tf.matmul(h, U_h) + b_h\n",
    "#     )\n",
    "#     # fully connected\n",
    "#     y_hat = tf.matmul(h, W_y) + b_y\n",
    "#     return y_hat, h #returning prediction and hidden state\n",
    "\n",
    "# @tf.function\n",
    "# def iterate_series(x, h):\n",
    "#     y_hat = []\n",
    "#     # iterate over time axis (1)\n",
    "#     for j in range(x.shape[1]):\n",
    "#         # give previous hidden state and input from the current time step\n",
    "#         y_hat_t, h = step(x[:, j], h)\n",
    "#         y_hat.append(y_hat_t)\n",
    "#     y_hat = tf.stack(y_hat, 1)\n",
    "#     return y_hat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "trainable_vars = [W_f, U_f, b_f, W_i, U_i, b_i, W_j, U_j, b_j, W_o, U_o, b_o]\n",
    "#trainable_vars.append(h_0, c_0)\n",
    "\n",
    "#trainable_vars = [W_h, U_h, b_h, W_y, b_y] #*for rnn\n",
    "# if rnn_mode:\n",
    "#     trainable_vars.append(U_h)\n",
    "# if learn_h0:\n",
    "#     trainable_vars.append(h_0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-6-32ef73aff365>:34 iterate_series  *\n        y_hat_t, h, c = step(x[:, j], h, c)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:568 __call__\n        result = self._call(*args, **kwds)\n    <ipython-input-6-32ef73aff365>:21 step  *\n        c = tf.matmul(c, f) + tf.matmul(j, i)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py:180 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:2798 matmul\n        a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py:5626 mat_mul\n        name=name)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:595 _create_op_internal\n        compute_device)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3322 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1786 __init__\n        control_input_ops)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 8 and 5 for 'MatMul_8' (op: 'MatMul') with input shapes: [5,8], [5,8].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9771319ca731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# get predictions for this part (forward pass)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m#y_hat, h = iterate_series(x_part, h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m# calculate mean squared error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 497\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-6-32ef73aff365>:34 iterate_series  *\n        y_hat_t, h, c = step(x[:, j], h, c)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:568 __call__\n        result = self._call(*args, **kwds)\n    <ipython-input-6-32ef73aff365>:21 step  *\n        c = tf.matmul(c, f) + tf.matmul(j, i)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py:180 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:2798 matmul\n        a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py:5626 mat_mul\n        name=name)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:595 _create_op_internal\n        compute_device)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3322 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1786 __init__\n        control_input_ops)\n    C:\\Users\\mahdi\\AppData\\Local\\Continuum\\anaconda3\\envs\\lsda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 8 and 5 for 'MatMul_8' (op: 'MatMul') with input shapes: [5,8], [5,8].\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "plt.ion()\n",
    "f, ax = plt.subplots(1)\n",
    "y_line, = ax.plot([],[], color=\"blue\", label=\"target $y$\")\n",
    "pred_line, = ax.plot([],[], color=\"red\", label=\"prediction $\\haty$\")\n",
    "input_line, = ax.plot([],[], color=\"green\", label=\"input $x$\")\n",
    "text = ax.text(0, 1, \"\", )\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# train for a set number of iterations\n",
    "for iteration in range(n_iterations):\n",
    "    # generates a long time series / normally loaded from dataset (e.g. stocks, weather)\n",
    "    x, y = generateData(signal_length, predict_ahead, signal_repeats, batch_size, noise_strength)\n",
    "    h = None\n",
    "    predictions = None\n",
    "    loss_list = []\n",
    "    grads = None\n",
    "    \n",
    "    # do not feed complete series, but chunks of it (truncated_backprop_length)\n",
    "    for i in range(0, total_series_length, truncated_backprop_length):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if h is None:\n",
    "                # initialize hidden state (h_0) -> new shape (batch_size, num_neurons)\n",
    "                h = tf.repeat(h_0, batch_size, 0)\n",
    "                c = tf.repeat(c_0, batch_size, 0) #added\n",
    "            x_part = x[:, i: i + truncated_backprop_length]\n",
    "            y_part = y[:, i: i + truncated_backprop_length]\n",
    "            \n",
    "            # get predictions for this part (forward pass)\n",
    "            #y_hat, h = iterate_series(x_part, h)\n",
    "            y_hat, h, c = iterate_series(x_part, h, c)\n",
    "        \n",
    "            # calculate mean squared error\n",
    "            loss = tf.reduce_mean((y_hat - y_part)**2)        \n",
    "        # backprop\n",
    "        if grads is None:\n",
    "            grads = tape.gradient(loss, trainable_vars)\n",
    "        else:\n",
    "            grads = grads + tape.gradient(loss, trainable_vars)\n",
    "                           \n",
    "        loss_list.append(loss)\n",
    "        # combine with previous predictions\n",
    "        predictions = tf.concat([predictions, y_hat], 1) if predictions is not None else y_hat\n",
    "    \n",
    "    # finally we are adapting the weights\n",
    "    optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        \n",
    "#     print(\"Iteration\", iteration, \"Loss\", np.mean(loss_list))  \n",
    "    replot(x[0], predictions[0], y[0], f, ax, input_line, y_line, pred_line, text, iteration, np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks in Keras (30 pts.)\n",
    "Add different components to the RNN from LSDA2020 RNN2.ipynb and test the new mod-\n",
    "els (each bullet point is a new separate model). The number of neuron can stay the same\n",
    "for all layers (default was 64). Report the mean validation error and the changed parts\n",
    "with a bit of context in the code (e.g. complete model or optimizer definition). Also,\n",
    "provide a model overview with model.summary() for each new model (not necessary for\n",
    "gradient clipping).\n",
    "1. (7 pts.) Replace the LSTM layer with a Bidirectional-LSTM. Bidirectional sequence processing is possible with tf.keras.layers.Bidirectional.\n",
    "2. (4 pts.) Stack 2 LSTM layers (you may need to use the return sequences parameter).\n",
    " * (4 pts.) Detail the difference to bidirectional processing (max. 2-3 sentences).\n",
    "3. (7 pts.) Add a 1-d convolution layer (tf.keras.layers.Conv1D) with a kernel size of 3 and stride of 1 before the recurrent part. You will need to reshape the data with tf.keras.layers.Reshape.\n",
    " * (5 pts.) Explain the difference between how a convolutional layer process time series and how a recurrent model does it (max 4-5 sentences).\n",
    "4. (3 pts.) Gradient clipping can be a helpful to train recurrent networks. Keras offers to clip gradients directly through the optimizer. Change the optimizer to clip the gradients to 1.\n",
    "It is more important here to show that you correctly changed the model (by showing the\n",
    "corresponding code in the report) or optimizer than improving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some layer\n",
    "from tensorflow.keras.layers import SimpleRNN, Input, Dense, LSTM, GRU, AveragePooling1D, TimeDistributed, Bidirectional, Conv1D, Flatten, Layer, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations, initializers\n",
    "INPUT_SHAPE = (100, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1\n",
    "def get_model_bidirectional(shape= INPUT_SHAPE):\n",
    "    inp = Input(shape)\n",
    "    x = Bidirectional(LSTM(64))(inp)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "\n",
    "get_model_bidirectional().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2\n",
    "def get_stacked_LSTM(shape= INPUT_SHAPE):\n",
    "    inp = Input(shape)\n",
    "    x = LSTM(64, return_sequences=True)(inp)\n",
    "    x = LSTM(64, go_backwards=True)(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "\n",
    "get_stacked_LSTM().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Detail the difference to bidirectional processing (max. 2-3 sentences\n",
    "The bidirectional layers outputs two concatenated LSTM layers, while the stacked layers output one single LSTM layer. In the stacked layer, the input of the backward layer is affected by the forward layer since it passes through the forward lyer, but the two layers are indepdedant in the bidirectional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a 1-d convolution layer (tf.keras.layers.Conv1D) with a kernel size of 3 and stride of 1 before the recurrent part. \n",
    "#You will need to reshape the data with tf.keras.layers.Reshape. ??????????\n",
    "# Exercise 4.3\n",
    "def get_model_convolutional(shape = INPUT_SHAPE):\n",
    "    inp = Input(shape)\n",
    "    x = Conv1D(filters=32, kernel_size=3, strides=1)(inp)\n",
    "    x = LSTM(64, return_sequences=False)(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "get_model_convolutional().summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent nueral network uses every piece of the input data to make a predictiion. A convolutional neural network extracts features from the input data a subset of the input data and uses the features to make a prediction. Not all input data may be necessary to makea good prediction therefore CNNs can be less comoutationally exensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, clipvalue=1)\n",
    "get_model_convolutional().compile(optimizer, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_rnn(shape = INPUT_SHAPE):\n",
    "    inp = Input(shape)\n",
    "    x = LSTM(64, return_sequences=False)(inp)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "get_model_rnn().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsda",
   "language": "python",
   "name": "lsda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
